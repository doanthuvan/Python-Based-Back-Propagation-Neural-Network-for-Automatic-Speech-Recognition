{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python based Back Propogation Neural Network for Automatic Speech Recognition\n",
    "## Group No. 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Likhit Teja Valavala  -  2015A3PS0221P\n",
    "#Shikhar Shiromani     -  2015A3PS0194P\n",
    "#Pratyush Priyank      -  2015A3PS0188P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.10586]\r\n",
      "(c) 2015 Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "(C:\\Users\\likhit\\Anaconda3) C:\\Users\\likhit\\Anaconda3>\n",
      "(C:\\Users\\likhit\\Anaconda3) C:\\Users\\likhit\\Anaconda3>pip install jdc\n",
      "Requirement already satisfied: jdc in c:\\users\\likhit\\anaconda3\\lib\\site-packages\r\n",
      "\r\n",
      "(C:\\Users\\likhit\\Anaconda3) C:\\Users\\likhit\\Anaconda3>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "\n",
    "pip install jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import random\n",
    "import numpy as np\n",
    "import jdc\n",
    "import sklearn\n",
    "from datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca(x,k):\n",
    "    covar_x = np.dot(x.T,x)/x.shape[0]\n",
    "    [U,S,V] = scipy.linalg.svd(covar_x)\n",
    "    Z = np.dot(x,U[:,0:k])\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "names = [\"_jackson_\",\"_theo_\",\"_jason_\"]\n",
    "for k in range(0,2):\n",
    "    for i in range(0,10):\n",
    "        for j in range(0,40):\n",
    "            string = str(i)+names[k]+str(j)+\".wav\"\n",
    "            (rate,sig) = wav.read(string)\n",
    "            mfcc_feat = mfcc(sig,rate,winlen=0.025,winstep=0.01,numcep=13,nfilt=26,nfft=1103,lowfreq=0,highfreq=None,preemph=0.97,\n",
    "                    ceplifter=22,appendEnergy=True)  \n",
    "            z = pca(mfcc_feat.T,1)\n",
    "            training_data.append(z)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for k in range(0,2):\n",
    "    for i in range(0,10):\n",
    "        temp = [0]*10\n",
    "        temp[i] = 1\n",
    "        temp = np.array([temp]).T\n",
    "        for j in range(0,40):\n",
    "            outputs.append(temp)\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "names = [\"_jackson_\",\"_theo_\",\"_jason_\"]\n",
    "for k in range(0,2):\n",
    "    for i in range(0,10):\n",
    "        for j in range(40,50):\n",
    "            string = str(i)+names[k]+str(j)+\".wav\"\n",
    "            (rate,sig) = wav.read(string)\n",
    "            mfcc_feat = mfcc(sig,rate,winlen=0.025,winstep=0.01,numcep=13,nfilt=26,nfft=1103,lowfreq=0,highfreq=None,preemph=0.97,\n",
    "                    ceplifter=22,appendEnergy=True)  \n",
    "            z = pca(mfcc_feat.T,1)\n",
    "            test_data.append(z)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "test_outputs = []\n",
    "for k in range(0,2):\n",
    "    for i in range(0,10):\n",
    "        temp = [0]*10\n",
    "        temp[i] = 1\n",
    "        temp = np.array([temp]).T\n",
    "        for j in range(0,10):\n",
    "            test_outputs.append(temp)\n",
    "print(len(test_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a generic neural network architecture as a python class which we would use in multiple exercies. \n",
    "\n",
    "**Note:** We are using jdc to define each method of `class Network` in seperate cells. jdc follows the following syntax,\n",
    "\n",
    "```py\n",
    "%%add_to #CLASS_NAME#\n",
    "def dummy_method(self):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network. For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.initialize_biases()\n",
    "        self.initialize_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "##  Initialize weights and biases\n",
    "\n",
    "The biases and weights for the network are initialized randomly, using a Gaussian\n",
    "distribution with mean 0, and variance 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def initialize_biases(self):\n",
    "    \n",
    "    self.biases = [np.random.uniform(-0.5,0.5,(self.sizes[b],1)) for b in range(1,self.num_layers)]\n",
    "    self.delta_b = [np.zeros((self.sizes[b],1)) for b in range(1,self.num_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def initialize_weights(self):\n",
    "    \n",
    "    self.weights = [np.random.uniform(-0.5,0.5,(self.sizes[b],self.sizes[b-1])) for b in range(1,self.num_layers)]\n",
    "    self.delta_w = [np.zeros((self.sizes[b],self.sizes[b-1])) for b in range(1,self.num_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We shall implement backpropagation with stochastic mini-batch gradient descent to optimize our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def train(self, training_data, epochs, mini_batch_size, learning_rate,momentum):\n",
    "    \"\"\"Train the neural network using gradient descent.  \n",
    "    ``training_data`` is a list of tuples ``(x, y)``\n",
    "    representing the training inputs and the desired\n",
    "    outputs.  The other parameters are self-explanatory.\"\"\"\n",
    "\n",
    "    # training_data is a list and is passed by reference\n",
    "    # To prevernt affecting the original data we use \n",
    "    # this hack to create a copy of training_data\n",
    "    # https://stackoverflow.com/a/2612815\n",
    "    training_data = list(training_data)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Get mini-batches    \n",
    "        mini_batches = self.create_mini_batches(training_data, mini_batch_size)\n",
    "        \n",
    "        # Itterate over mini-batches to update pramaters   \n",
    "        cost = sum(map(lambda mini_batch: self.update_params(mini_batch, learning_rate,momentum), mini_batches))\n",
    "        \n",
    "        # Find accuracy of the model at the end of epoch         \n",
    "        acc = self.evaluate(training_data)\n",
    "        \n",
    "        if(i%100==0):\n",
    "            print(\"Epoch {} complete. Total Accuracy: {}\".format(i,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mini-batches\n",
    "\n",
    "Split the training data into mini-batches of size `mini_batch_size` and return a list of mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def create_mini_batches(self, training_data, mini_batch_size):\n",
    "    # Shuffling data helps a lot in mini-batch SGD\n",
    "    random.shuffle(training_data)\n",
    "    # YOUR CODE HERE\n",
    "    mini_batches = [training_data[k:k+mini_batch_size] for k in range(0,len(training_data),mini_batch_size)]\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Update weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def update_params(self, mini_batch, learning_rate,momentum):\n",
    "    \"\"\"Update the network's weights and biases by applying\n",
    "    gradient descent using backpropagation.\"\"\"\n",
    "    #print(mini_batch)\n",
    "    # Initialize gradients     \n",
    "    delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    for x,y in mini_batch:\n",
    "        # Obtain the mean squared error and the gradients\n",
    "        # with resepect to biases and weights \n",
    "        \n",
    "        cost, del_b, del_w = self.backprop(x, y)\n",
    "        \n",
    "        # Add the gradients for each sample in mini-batch        \n",
    "        delta_b = [nb + dnb for nb, dnb in zip(delta_b, del_b)]\n",
    "        delta_w = [nw + dnw for nw, dnw in zip(delta_w, del_w)]\n",
    "        \n",
    "        total_cost += cost\n",
    "\n",
    "    # Update self.biases and self.weights\n",
    "    # using delta_b, delta_w and learning_rate \n",
    "    #Momentum\n",
    "    self.delta_b = [(learning_rate*delta + momentum*db) for delta,db in zip(delta_b,self.delta_b)]\n",
    "    self.biases = [b - (1 / len(mini_batch)) * db\n",
    "                   for b, db in zip(self.biases, self.delta_b)]\n",
    "    self.delta_w = [(learning_rate*delta + momentum*dw) for delta,dw in zip(delta_w,self.delta_w)]\n",
    "    self.weights = [w - (1 / len(mini_batch)) * dw\n",
    "                    for w, dw in zip(self.weights, self.delta_w)]\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def backprop(self, x, y):\n",
    "    \"\"\"Return arry containiing cost, del_b, del_w representing the\n",
    "    cost function C(x) and gradient for cost function.  ``del_b`` and\n",
    "    ``del_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "    to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "    # Forward pass\n",
    "    zs, activations = self.forward(x)\n",
    "    \n",
    "    # Backward pass     \n",
    "    cost, del_b, del_w = self.backward(activations, zs, y)\n",
    "\n",
    "    return cost, del_b, del_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def sigmoid(self, z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def sigmoid_derivative(self, z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return self.sigmoid(z)*(1-self.sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propogration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def forward(self, x):\n",
    "    \"\"\"Compute Z and activation for each layer.\"\"\"\n",
    "    \n",
    "    # list to store all the activations, layer by layer\n",
    "    zs = []\n",
    "    \n",
    "    # current activation\n",
    "    activation = x\n",
    "    # list to store all the activations, layer by layer\n",
    "    activations = [x]\n",
    "    \n",
    "    # Loop through each layer to compute activations and Zs  \n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        # YOUR CODE HERE\n",
    "        # Calculate z\n",
    "        # watch out for the dimensions of multiplying matrices\n",
    "        #print(w)\n",
    "        #print(activations[-1])\n",
    "        z = np.dot(w,activations[-1])+b\n",
    "        zs.append(z)\n",
    "        # Calculate activation\n",
    "        activation = self.sigmoid(z)\n",
    "        activations.append(activation)\n",
    "        \n",
    "    return zs, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loss Function\n",
    "Logistic regression error and  it's derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def lre(self, output_activations, y):\n",
    "    \"\"\"Returns mean square error.\"\"\"\n",
    "    return -(y*np.log(output_activations) + (1-y)*np.log(1-output_activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def lre_derivative(self, output_activations, y):\n",
    "    \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "    \\partial a for the output activations. \"\"\"\n",
    "    return -(y/output_activations - (1-y)/(1-output_activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def backward(self, activations, zs, y):\n",
    "    \"\"\"Compute and return cost funcation, gradients for \n",
    "    weights and biases for each layer.\"\"\"\n",
    "    # Initialize gradient arrays\n",
    "    del_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    del_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    \n",
    "    # Compute for last layer\n",
    "    cost = self.lre(activations[-1], y)\n",
    "    \n",
    "    delta = self.lre_derivative(activations[-1],y)*self.sigmoid_derivative(zs[-1])\n",
    "    #print(delta.shape)\n",
    "    del_b[-1] = delta\n",
    "    del_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    #print(del_w[-1].shape)\n",
    "    \n",
    "    # Loop through each layer in reverse direction to \n",
    "    # populate del_b and del_w   \n",
    "    for l in range(2, self.num_layers):\n",
    "        #print(delta.shape);print(self.sigmoid_derivative(activations[-l]).shape); print(np.dot(self.weights[-l+1].T,delta).shape)\n",
    "        delta = np.dot(self.weights[-l+1].T,delta)*self.sigmoid_derivative(zs[-l])\n",
    "        #print(delta.shape)\n",
    "        del_b[-l] = delta\n",
    "        del_w[-l] = np.dot(delta, activations[-l -1].transpose())\n",
    "        #print(del_w[-l].shape)\n",
    "    \n",
    "    return cost, del_b, del_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def evaluate(self, test_data):\n",
    "    \"\"\"Return the accuracy of Network. Note that the neural\n",
    "    network's output is assumed to be the index of whichever\n",
    "    neuron in the final layer has the highest activation.\"\"\"\n",
    "    test_results = [(np.argmax(self.forward(x)[1][-1]), np.argmax(y))\n",
    "                    for (x, y) in test_data]\n",
    "    return sum(int(x == y) for (x, y) in test_results) * 100 / len(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showtime\n",
    "\n",
    "Let's test our implementation on a bunch of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete. Total Accuracy: 10.875\n",
      "Epoch 100 complete. Total Accuracy: 71.625\n",
      "Epoch 200 complete. Total Accuracy: 76.875\n",
      "Epoch 300 complete. Total Accuracy: 77.75\n",
      "Epoch 400 complete. Total Accuracy: 78.375\n",
      "Epoch 500 complete. Total Accuracy: 78.125\n",
      "Epoch 600 complete. Total Accuracy: 78.375\n",
      "Epoch 700 complete. Total Accuracy: 78.5\n",
      "Epoch 800 complete. Total Accuracy: 78.25\n",
      "Epoch 900 complete. Total Accuracy: 79.5\n",
      "Epoch 1000 complete. Total Accuracy: 78.5\n",
      "Epoch 1100 complete. Total Accuracy: 79.625\n",
      "Epoch 1200 complete. Total Accuracy: 79.625\n",
      "Epoch 1300 complete. Total Accuracy: 79.125\n",
      "Epoch 1400 complete. Total Accuracy: 79.625\n",
      "Epoch 1500 complete. Total Accuracy: 79.625\n",
      "Epoch 1600 complete. Total Accuracy: 79.5\n",
      "Epoch 1700 complete. Total Accuracy: 79.25\n",
      "Epoch 1800 complete. Total Accuracy: 79.125\n",
      "Epoch 1900 complete. Total Accuracy: 79.625\n",
      "Epoch 2000 complete. Total Accuracy: 79.125\n",
      "Epoch 2100 complete. Total Accuracy: 79.0\n",
      "Epoch 2200 complete. Total Accuracy: 79.5\n",
      "Epoch 2300 complete. Total Accuracy: 79.0\n",
      "Epoch 2400 complete. Total Accuracy: 79.5\n",
      "Epoch 2500 complete. Total Accuracy: 79.125\n",
      "Epoch 2600 complete. Total Accuracy: 79.5\n",
      "Epoch 2700 complete. Total Accuracy: 79.5\n",
      "Epoch 2800 complete. Total Accuracy: 79.5\n",
      "Epoch 2900 complete. Total Accuracy: 79.75\n",
      "Epoch 3000 complete. Total Accuracy: 79.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likhit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "training_data = [sklearn.preprocessing.normalize(a) for a in training_data]\n",
    "data = list(zip(training_data,outputs))\n",
    "#print(data)\n",
    "network = Network([13, 11,8, 10])\n",
    "network.train(data,3001,100,1,0.2)\n",
    "#network.evaluate(list(zip(test_data,test_outputs)))\n",
    "predictions = list(map(lambda sample: np.argmax(network.forward(sample)[1][-1]), test_data))\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likhit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51.5"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.evaluate(list(zip(test_data,test_outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
